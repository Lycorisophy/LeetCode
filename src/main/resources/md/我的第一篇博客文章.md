# **基于双维注意力机制的事件要素识别方法**

廖涛，宋杨，张顺香

(安徽理工大学，计算机科学与工程学院，安徽 淮南 232001)

**摘 要：** 事件要素识别是事件抽取的基本任务，对后续任务的开展起着重要作用。随着深度学习技术的发展，神经网络逐渐成为事件要素识别的主流方法。近几年，注意力机制在自然语言处理领域得到广泛使用。但注意力机制依赖于嵌入层对上下文特征的提取，在事件要素识别等序列标注问题中效果不理想。针对这一问题，提出了一种基于注意力机制的双维注意力机制，它从矩阵行和列两个维度计算注意力得分，因此可以较好地提取句子的上下文特征。并且使用了动态目标损失函数，通过动态给予不同标签不同的权重，一定程度上缓解了样本不平衡和数据稀疏带来的模型泛化不足问题。在CEC2.0中文突发事件语料库上对比实验表明，所提方法取得较好效果。

**关键词：** 事件要素识别；神经网络；序列标注任务；双维注意力机制；动态目标损失函数

**中图分类号：** TP391

**Event Element Identification Method Based on Bi-dimension Attention Mechanism**

LIAO Tao, SONG Yang, ZHANG Shun-xiang

(School of Computer Science and Engineering, Anhui University of Science &amp; Technology, 232001, China)

**【**** Abstract ****】** Event element identification is the basic task of event extraction and plays an important role in the development of subsequent tasks. With the development of deep learning technology, neural network has gradually become the mainstream method of event element identification. In recent years, attention mechanisms have been widely used in natural language processing. However, the attention mechanism depends on the extraction of context features by the embedding layer, which is not ideal in sequence labeling tasks such as event element recognition. To solve this problem, bi-dimension attention mechanism based on attention mechanism is proposed. It calculates the attention score from two dimensions of matrix row and column, so it can better extract the context features of sentences. In addition, dynamic target loss function is used to give different weights to different labels dynamically, which alleviates the lack of model generalization caused by sample imbalance and data sparseness to some extent. Comparative experiments on CEC2.0 Chinese emergency corpus show that the proposed method achieves good results.

**【**** Key words ****】** event element identification; neural network; sequence labeling task; bi-dimension attention mechanism; dynamic target loss function

# 0 引言

作为事件研究领域的基本任务，事件要素识别被广泛应用于信息检索、文本翻译、推荐系统等自然语言处理任务中。因此，事件要素识别技术及其理论研究受到越来越多研究机构和研究者的关注。如MUC（Message Understanding Conference）会议和ACE（Automatic Content Extraction）会议都是典型的含有事件要素识别任务的评测会议。

根据ACE会议和LIU[1-2]等的定义，事件由事件触发词（或标志词）和描述事件结构的其他要素构成。事件要素识别旨在从非结构化的含有事件的文本中自动识别其中的时间、地点、动作等重要信息，并给予其准确的角色标注。以下给出一个事件要素识别的实例。

S1：下午3点，救援组官兵使用破拆工具对长安车进行破拆。

对于事件句S1，事件要素识别任务需识别出该句中第二个&quot;破拆&quot;为标志词，时间为&quot;下午3点&quot;，人物为&quot;救援组官兵&quot;，对象为&quot;长安车&quot;，工具或方法为&quot;破拆工具&quot;。

传统的机器学习方法[3-6]将事件要素识别建模成分类任务，侧重于设计有效特征或核函数。基于特征的算法，如决策树等，在特定领域内可以取得较好效果，但规则的可移植性差，难以获取复杂句或跨句的模式。为此提出了基于核函数的方法，在英文数据集上取得了一定效果，但不适合结构相对英文更加松散的中文数据集。

基于深度学习的神经网络方法[7-9]，如RNNs和CNNs等，逐渐成为机器学习的主流技术。尤其是近几年，随着注意力机制和Transformer架构被用于深度学习[11-14]，注意力机制并行速度快且能直接建立长距离依赖关系，在一定程度上加速了神经网络对特征的提取，特别是事件要素识别等自然语言处理领域。但是，注意力机制依赖于嵌入层语言模型对上下文特征的提取，缺乏对文本上下文特征的独立提取能力，因此在小数据集上表现稍差。

本文针对Transformer框架特征提取层（Encoder层）依赖于嵌入层（Embedding层）对上下文特征的提取，提出了一种基于注意力机制的双维注意力机制用于特征提取层。并针对本文所用中文语料相对结构松散和数据稀疏的问题，使用了基于软目标的动态目标损失函数。文本将这些结合起来构建了事件要素识别方法。实验结果表明，本文方法在CEC2.0中文突发事件语料库上表现出优于当前主流模型的性能。

# 1相关工作

2006年，Ahn[3]提出把事件要素识别当作多元分类任务，在ACE英文语料上取得了不错效果，但不适用于中文语料。ZHAO[4]对此进行了改进，通过构造多元分类模型的方法进行事件要素识别，在ACE中文语料上效果有所提高。FU[5]针对监督方法依赖于语料的质量和规模，提出了基于特征加权的无监督聚类算法进行事件要素识别，在生语料上取得了不错的效果。LIU[6]等针对数据稀疏问题，提出一种基于事件本体的文本事件要素提取方法，大大降低了对语料的依赖。

CHEN[7]等利用动态多池化的卷积神经网络方法抽取一个句子中多个事件触发词。FENG[8]利用双向长短期记忆神经网络（Bi-LSTM）进行事件检测。Nguyen[9]使用Bi-LSTM提取文本中的语义信息，并联合文本结构特征进行事件要素识别。

谷歌AI团队在2017年提出了结合自注意力机制的Transformer文本架构[10]，并于2018年在此基础上提出了BERT语言模型[11]，在多项NLP任务中取得优异效果。SHEN[12]等提出一种结合注意力机制与Bi-LSTM的中文事件检测模型，将事件要素识别任务看作一种序列标注任务，在ACE中文数据集上获得了非常好的效果。HUANG[13]提出了动态掩蔽注意力机制，与常规注意力机制相比能够捕捉更丰富的上下文表示。TANG[14]提出一种结合双向门控单元和多头注意力的序列标注方法，用于预测电子病例中的命名实体，并结合条件随机场预测全局最优的标签序列，该方法在标准数据集上取得了良好的结果。

本文提出了结合双维注意力机制和动态目标损失函数的神经网络模型（BDAtt-DT模型）用于事件要素识别，本文贡献如下：

1）把事件要素识别作为序列标注任务处理；

2）提出并使用了双维注意力机制加强了对文本上下文特征的提取；

3）使用了动态目标损失函数一定程度上缓解了数据稀疏和中文结构相对松散带来的模型泛化不足问题；

4）将双维注意力机制和动态目标损失函数等结合使用，构建了神经网络模型用于事件要素识别。

# 2 BDAtt-DT模型

本文将事件要素识别当作序列标注问题，即为句子中每一个字或符号确定一个相应的标签。并提出了BDAtt-DT（双维注意力机制和动态目标）模型，结合双维注意力机制和动态目标损失函数等用于事件要素识别。模型整体结构如图1所示。

![](RackMultipart20220206-4-1pb8gg7_html_10c23950343f519c.gif)

**图**** 1 BDA ****tt**** -DT ****模型结构图**

该模型大体分为四个部分：

1）特征嵌入层：经过预处理后的语料以句为单位通过Embedding网络，映射成特征矩阵 **M**** t**，输入到下一层中；

2）特征提取层：该层结合双向注意力机制从矩阵 **M**** t **行和列两个方向提取特征，得到文本矩阵** M ****e** ；

3）输出层：该层使用上一层得到的文本矩阵 **M**** e**，通过分类网络得到预测的训练标注结果；

4）反向传播层：该层结合输出层得到的预测结果，使用动态目标机制得到动态目标，并经过损失函数运算后得到损失值，从而进行误差反向传播，更新模型参数。

## （1） 特征嵌入层

语料库中的文本数据首先经过BIO标注和数据清洗等预处理工作，BIO标注将每个元素标注为&quot;B-X&quot;、&quot;I-X&quot;或者&quot;O&quot;。其中，&quot;B-X&quot;表示此元素所在的片段属于X类型并且此元素在此片段的开头，&quot;I-X&quot;表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，&quot;O&quot;表示不属于任何类型。

本文使用了Albert中文预训练语言模型[15]得到文本特征矩阵Mt，并经过归一化处理和dropout防止过拟合，Albert是基于BERT语言模型[11]的改进，同样使用Transformer[10]进行模型训练。

## （2） 特征提取层

该层结合了双维注意力机制用于从特征矩阵 **M**** t**行和列两个维度提取有效特征，双维注意力机制是对注意力机制的改进。

注意力机制能很好地提取字粒度的特征，但缺少了对上下文特征的提取。注意力机制依赖于特征嵌入层对上下文特征的嵌入，所以注意力机制在事件要素识别这类依赖上下文特征的任务中无法取得较好效果。双维注意力机制整体架构如图2所示。

![](RackMultipart20220206-4-1pb8gg7_html_a7e62ba202dbde16.gif)

**图**** 2 **** 双维注意力机制**

其中上下文注意力网络进一步提取文本的上下文特征，该网络首先将特征矩阵 **M**** t **转置，再通过３个不同的线性变换将矩阵映射成矩阵** Q **、** K **和** V**, 再利用式(1)计算注意力得分，作为上下文矩阵**M ****c.**

| ![](RackMultipart20220206-4-1pb8gg7_html_afa9ee21545f8c43.gif) |
# (1)
|
| --- | --- |

其中， ![](RackMultipart20220206-4-1pb8gg7_html_447b87d7da33944b.gif)根据经验设定，起到平滑的作用。

字粒度注意力网络提取文本中的字粒度特征，计算字粒度注意力时使用的是多头自注意力机制[10]，多头注意力机制首先通过３个不同的线性变换将矩阵 **M**** t **映射成矩阵** Q **,** K **和** V **,再将矩阵** Q **,** K **、** V**进行等比拆分，然后将每个子矩阵输入到放缩点积Attention，如式（2）所示。

| ![](RackMultipart20220206-4-1pb8gg7_html_d68f6f35f683711e.gif) |
# (2)
|
| --- | --- |

然后将_h_次的放缩点积Attention结果_h__i_进行拼接，再进行一次线性变换,得到字粒度矩阵 **M**** w**,如式（3）所示。

| ![](RackMultipart20220206-4-1pb8gg7_html_966b71e604900a7e.gif) |
# (3)
|
| --- | --- |

输出时将上下文矩阵 **M**** c **的转置和字粒度矩阵Mw进行列拼接，得到文本矩阵** M ****e** ，如式（4）所示。

|
 | ![](RackMultipart20220206-4-1pb8gg7_html_360f77d793eee68d.gif) |
# (4)
|
| --- | --- | --- |

## （3） 输出层

输出层中分类网络由线性全连接网络，激活函数和softmax函数组成,激活函数的计算公式如式（5）所示。

|
 | ![](RackMultipart20220206-4-1pb8gg7_html_5dc73766b1171c4b.gif) |
# (5)
|
| --- | --- | --- |

## （4） 反向传播层

该层在计算损失函数时使用了动态目标机制。通常的目标函数使用one-hot编码，这种目标函数也称为硬目标（hard target），硬目标只有在目标位置的值为1，其余为0.而软目标[16]（soft target）在其余部分也会有概率，所以比硬目标包含的信息熵更高。

动态目标（dynamic target）的目标位置 ![](RackMultipart20220206-4-1pb8gg7_html_a1a7790fd37f6b6.gif),其余为1。其中，_d __ij_为序列中第i个样本的第_j_个标签，_i_最大为序列最大长度128，_s__ j_为第_j_个标签的分数，_s__j_∈[0,+∞],分数越高，该标注区分度越大，默认_s__j_ = 10，表示所有事件要素的重要度都为10。_t_为软化率，_t_∈[0,1]。_t_=0时，目标的信息熵最大，_t_ = 1时，目标相当于硬目标。t的计算公式如式（6）所示。

|
 | ![](RackMultipart20220206-4-1pb8gg7_html_ad5c0803a7f66c8b.gif) |
# (6)
|
| --- | --- | --- |

设T为标签类别数，目标的损失函数定义为交叉熵和均方误差的加权平均，如式（7）所示。

|
 | ![](RackMultipart20220206-4-1pb8gg7_html_330e71b7adad2fb1.gif) |
# (7)
|
| --- | --- | --- |

其中，_x_为预测值，_y_为实际值，_α_根据经验设定，可以防止模型过拟合，本文中_α_ = 0.8.

# 3实验与分析

## （1） 数据集及评价指标

本文中使用的是CEC2.0中文突发事件语料库[17]。CEC2.0收集了5类合计332篇（地震、火灾、交通事故、恐怖袭击和食物中毒）突发事件新闻报道，采用了XML格式进行标注，其中事件要素标注包括时间、地点、对象、参与者、工具或方法，各要素含义及其BIO标注数量如表1所示。

表1 CEC２.０语料库上定义的事件要素

|
# 中文
|
# 英文
|
# 数量
|
# 说明
|
| --- | --- | --- | --- |
|
# 标志词
|
# Denoter
|
# 6026
|
# 标志事件发生的触发词
|
| --- | --- | --- | --- |
|
# 时间
|
# Time
|
# 5434
|
# 事件发生或持续的时间
|
|
# 地点
|
# Location
|
# 7926
|
# 事件发生的地点
|
|
# 参与者
|
# Paticipant
|
# 8203
|
# 参与事件事件的人或团体
|
|
# 对象
|
# Object
|
# 5002
|
# 被事件影响的对象
|
|
# 工具或方法
|
# Means
|
# 224
|
# 事件中所用的工具或方法
|

通过Ｆ1值（F1-Measure）评价序列标注的效果。Ｆ１值为准确率和召回率的调和平均值，计算方法如公式（8）所示。

| ![](RackMultipart20220206-4-1pb8gg7_html_7023129f46bf69ce.gif) |
# (8)
|
| --- | --- |

其中，精确率是计算正确标注某要素数占该要素实际总数的比例。召回率用来计算正确标注某要素数占该要素预测总数的比例。此外，非要素类标签，如Other和标注句子边界的标签不纳入计算。

## **（**** 2 ****） 实验结果**** 和分析**

在实验中，本文选用当前命名实体识别主流模型双向长短期记忆网络（Bi-LSTM）作为参照模型。LSTM作为RNN的一种变体，相比较于RNN可以学习到文本中的长期依赖关系。Bi-LSTM在LSTM的基础上，可以从前后两个方向提取文本上下文特征。相关研究[8-9,18-19]表明，Bi-LSTM是序列标注任务中的主流模型。

本文使用的部分参数设置见表2,其中词向量维度由预训练模型决定，其他参数根据实验经验设定。

表2参数设置

|
# 参数名称
|
# 值
|
| --- | --- |
|
# 词向量维度
|
# 1024
|
| --- | --- |
|
# 文本最大长度
|
# 128
|
|
# Epochs
|
# 20
|
|
# Bi-LSTM层数
|
# 4
|
|
# 双维注意力层数
|
# 4
|
|
# 优化器
|
# Adam
|
|
# 初始学习率
|
# 10-4
|

本实验每次将数据集随机打乱后按照4：1划分为训练集和测试集，各进行10次实验后将结果取平均值。各模型事件要素识别的F1值比较如表3所示。

表3各模型要素识别效果F1值比较

| 模型 | Denoter | Time | Location | Participant | Object | Means |
| --- | --- | --- | --- | --- | --- | --- |
| Bi-LSTM | 44.72 | 71.55 | 26.01 | 49.02 | - | - |
| --- | --- | --- | --- | --- | --- | --- |
| Transformer(使用自注意力机制) | 57.58 | 81.04 | 56.59 | 63.12 | 31.41 | 2.5 |
| **双维注意力** | **58.72** | **81.58** | **61.17** | **63.46** | **33.50** | **11.50** |
| Bi-LSTM+双维注意力 | 53.44 | 80.08 | 57.71 | 51.63 | 29.84 | - |

实验结果表明，使用双维注意力机制的模型与使用Bi-LSTM的模型以及Transformer架构相比能够更好地提取文本的上下文特征，更适合序列标注任务。同样对于前文中提到的事件句&quot;下午3点，救援组官兵使用破拆工具对长安车进行破拆&quot;进行要素识别，使用Bi-LSTM的模型错误地将&quot;破拆工具&quot;中的&quot;破拆&quot;标注为标志词，而本文中的模型则没有出现这样的错误。

其次，双维注意力机制加上双向长短期记忆网络并不能带来模型整体性能的提高，反而导致模型过拟合。模型在测试集上的精确率有所提升，但召回率有所下降，最后导致F1值反而下降。

各模型加入动态目标后的要素识别效果如表4所示。

表4动态目标对各模型要素识别效果F1值的影响

| 模型 | Denoter | Time | Location | Participant | Object | Means |
| --- | --- | --- | --- | --- | --- | --- |
| Bi-LSTM | 44.72 | 71.55 | 26.01 | 49.02 | - | - |
| --- | --- | --- | --- | --- | --- | --- |
| Bi-LSTM+动态目标 | 47.24 | 79.14 | 42.92 | 50.49 | 23.17 | - |
| 双维注意力 | 58.72 | 81.58 | **61.17** | **63.46** | 33.50 | 11.50 |
| **双维注意力**** + ****动态目标（**** BDAtt-DT ****）** | **58.92** | **82.22** | 60.99 | 62.79 | **34.90** | **14.33** |
| Bi-LSTM+双维注意力 | 53.44 | 80.08 | 57.71 | 51.63 | 29.84 | - |
| Bi-LSTM+双维注意力+动态目标 | 56.90 | 80.62 | 58.60 | 60.82 | 32.41 | 7.90 |

因为训练语料中样本分布不均匀，导致模型偏向于样本数多的标签。表中的实验结果表明，动态目标机制在一定程度上缓解了这一问题。尤其是对于样本量最少的Means要素和第二少的Object要素，使用了动态目标机制之后，模型对于这两种要素的识别效果提升显著。

研究还发现，降低某要素标签分数s_j_后，该要素自身精确率提高，其他要素精确率下降，自身召回率下降，其他召回率提高，反之亦反。所以，可以通过调整基本分数改变对不同事件要素的学习程度。

#

#

# 4 结束语

本文针对传统事件要素识别方法所存在的缺点，提出了一种基于双维注意力机制特征提取模型，并结合动态目标损失函数用于事件要素识别。实验结果表明，使用双维注意力机制使得模型能够更好地提取文本的上下文特征，更适合序列标注任务。其次，本文使用的动态目标损失函数，能够一定程度上提高模型的泛化能力。

本文提出的方法还有很多不足，例如该方法对于语料库中专有名词的识别仍然存在难度，以及该方法尚未在英文语料库中验证有效性。这些问题都亟待进一步深入研究和验证。

# 参考文献

[1]刘宗田，黄美丽，周文等.面向事件的本体研究[J].计算机科学.2009，11(36).

[2] 廖 涛，刘宗田，王先传.基于事件的文本表示方法研究[J].计算机科学.2012，39(12).

[3] AHN D.The stages of event extraction[C]//Proceedings of the COLING/ACL 2006 Workshop on Annotation and Reasoning About Time and Events.2006:1-8.

[4] 赵妍妍，秦兵，车万翔等.中文事件抽取技术研究[J].中文信息学报.2008，22(1).

[5] 付剑锋，刘宗田，刘炜等.基于特征加权的事件要素识别[J].计算机科学.2010，37(3).

[6] 刘 炜，刘菲京，王 东等.一种基于事件本体的文本事件要素提取方法[J].中文信息学报.2016，30(4).

[7] CHEN Y, XU L, LIU K, etc.Event extraction via dynamic multi-pooling convolutional neural networks[C]//Proceedings of the 53rd ACL and the 7th IJCNLP.Beijing,China:ACL,2015:167-176.

[8] FENG X, QIN B, LIU T.A language-independent neural network for event detection[J].Science China-Information Sciences.2018,61(9):092106.

[9] NGUYEN T H, CHO K, GRISHMAN R.Joint event extraction via recurrent neural networks[C]//Proceedings of the 2016 NAACL:human language technologies.San Diego, California:ACL,2016:300-309.

[10] VASWANI A, SHAZEER N, PARMAR N, etc. Attention is all you need[C]//Proceedings of the 2017 Annual Conference on Neural Information Processing Systems, Long Beach, Dec 4-9, 2017. Red Hook: Curran Associates, 2017: 5998-6008.

[11] DEVLIN J, CHANG M, LEE K, etc.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.https://arxiv.org/pdf/1810.04805.pdf

[12] 沈兰奔, 武志昊, 纪宇泽等.结合注意力机制与双向LSTM的中文事件检测方法[J].中文信息学报,2019, 33(9).

[13] 黄细凤.基于动态掩蔽注意力机制的事件抽取[J].计算机应用研究.2020,7:1964-1968.

[14] 唐国强, 高大启, 阮彤等.融入语言模型和注意力机制的临床电子病历命名实体识别[J].计算机科学.2020,3:211-216.

[15] LAN Z, CHEN M, GOODMAN S, etc.ALBERT:A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATION[C]//ICLR,2020.

[16] HINTON G, VINYALS O, DEAN J.Distilling the Knowledge in a Neural Network[C]//NIPS 2014 Deep Learning Workshop.

[17] WANG X, LIU Z, LIAO T, etc.CEC2.0-Based Detection Event Relation for Chinese Text[C]//Proceedings of the 2015 International Conference on Materials Engineering and Information Technology Applications.

[18] HUANG Z, XU W, YU K. Bidirectional LSTM-CRF Models for Sequence Tagging[J]. Computer Science, 2015.

[19]温秀秀，马 超，高原原等.基于标签聚类的中文重叠命名实体识别方法[J].计算机工程.2020,5:41-46.

作者及联系方式：

廖涛，男，副教授，博士学历，硕士生导师。安徽省计算机学会会员，淮南市计算机领域评标专家。主要研究方向为Web数据挖掘、智能信息处理。主持安徽省高等学校优秀青年人才基金项目1项，安徽高校优秀青年人才支持计划项目1项，安徽高校自然科学研究重点项目1项。作为主要骨干，参与国家自然科学基金面上项目3项，国家自然科学基金主任基金项目1项，国家发改委经济动员中心项目1项，上海市科学技术委员会重点项目1项。在国际期刊、领域较重要的国际会议以及国内核心期刊发表论文20余篇。

通信地址：安徽省淮南市泰丰大街168号安徽理工大学计算机科学与工程学院邮编：232001

手机：18963777708

邮箱：tliao@aust.edu.cn

宋杨，男，硕士研究生，主要研究方向为事件抽取和自然语言处理。

导师：廖涛

手机：15216839375

邮箱：s2295938761@163.com

姓名：张顺香

手机：18963777827

邮箱：[sxzhang@aust.edu.cn](mailto:sxzhang@aust.edu.cn)

# 参考文献

[1]刘宗田,黄美丽,周文,等.面向事件的本体研究[J].计算机科学.2009,36(11):189-199.

[2] 廖涛,刘宗田,王先传.基于事件的文本表示方法研究[J].计算机科学.2012,39(12):188-191.

[3] AHN D.The stages of event extraction[C]//Proceedings of the Workshop on Annotating and Reasoning about Time and Events.Sydney,Australia:Association for Computational Linguistics,2006,1–8.

[4] 赵妍妍,秦兵,车万翔,等.中文事件抽取技术研究[J].中文信息学报.2008,22(1):3-8.

[5] 付剑锋,刘宗田,刘炜,等.基于特征加权的事件要素识别[J].计算机科学.2010,37(3):239-241.

[6] 刘炜,刘菲京,王东,等.一种基于事件本体的文本事件要素提取方法[J].中文信息学报.2016,30(4):167-171.

[7] CHEN Y,XU L,LIU K,et al.Event extraction via dynamic multi-pooling convolutional neural networks[C]//Proceedings of the 53rd ACL and the 7th IJCNLP.Beijing,China:Association for Computational Linguistics,2015,167-176.

[8] FENG X,QIN B,LIU T.A language-independent neural network for event detection[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.Berlin,Germany:Association for Computational Linguistics,2016,66-71.

[9] NGUYEN T H,CHO K,GRISHMAN R.Joint event extraction via recurrent neural networks[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,San Diego,California:Association for Computational Linguistics,2016,300-309.

[10] VASWANI A,SHAZEER N,PARMAR N,et al.Attention is all you need[C]//Proceedings of the 31st International Conference on Neural Information Processing Systems,Long Beach,CA,USA:Conference on Neural Information Processing Systems,2017,6000-6010.

[11] DEVLIN J,CHANG M,LEE K,et al.BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[J].arXiv preprint arXiv:1810.04805,2018.

[12] 沈兰奔,武志昊,纪宇泽,等.结合注意力机制与双向LSTM的中文事件检测方法[J].中文信息学报,2019, 33(9):79-87.

[13] 黄细凤.基于动态掩蔽注意力机制的事件抽取[J].计算机应用研究.2020,37(7):1964-1968.

[14] 唐国强,高大启,阮彤,等.融入语言模型和注意力机制的临床电子病历命名实体识别[J].计算机科学.2020,47(3):211-216.

[15] LAN Z,CHEN M,GOODMAN S,et al.ALBERT:A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATION[J].arXiv preprint arXiv:1909.11942,2019.

[16] HINTON G,VINYALS O,DEAN J.Distilling the Knowledge in a Neural Network[J].arXiv preprint arXiv:1503.02531,2015.

[17] WANG X,LIU Z,LIAO T,et al.CEC2.0-Based Detection Event Relation for Chinese Text[C]//Proceedings of the 2015 International Conference on Materials Engineering and Information Technology Applications:Atlantis Press,2015,862-865.

[18] HUANG Z,XU W,YU K.Bidirectional LSTM-CRF Models for Sequence Tagging[J].arXiv preprint arXiv:1508.01991,2015.

[19]温秀秀,马超,高原原,等.基于标签聚类的中文重叠命名实体识别方法[J].计算机工程.2020,46(5):41-46.

————————————

**基金项目：** 国家自然科学基金面上项目（62076006）；安徽省高校优秀青年人才支持计划项目（gxyq2017007)；安徽省高等学校自然研究重点项目（KJ2016A202）。

**作者简介：** 廖涛，男，副教授，主研领域：数据挖掘，智能信息处理。宋杨，硕士研究生。张顺香，教授。

**E-mail**** ：**s2295938761@163.com